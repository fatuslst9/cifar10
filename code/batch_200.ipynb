{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch import optim\nfrom torch import Tensor\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom torchvision.models import resnet\nfrom torchvision.models.resnet import Bottleneck\nfrom torchvision.models.resnet import BasicBlock\nfrom torchvision.models.resnet import resnet18\n\nfrom typing import Type, Any, Callable, Union, List, Optional\n\nimport time\n\n\"\"\"\n변경사항 num_class=10\n\"\"\"\n        \nclass ResNet(nn.Module):\n        \n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 10,\n        zero_init_residual: bool = False,\n        groups: int = 1,\n        width_per_group: int = 64,\n        replace_stride_with_dilation: Optional[List[bool]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=4, stride=1, padding=1,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n\n    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        \n        \"\"\"\n        변경사항 maxpool 제거\n        \"\"\"\n#         x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n  \n     \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n\n\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=200, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=2)\n\n\n\nmixup_alpha = 1.0\n\ndef mixup_data(x, y):\n    lam = np.random.beta(mixup_alpha, mixup_alpha)\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).cuda()\n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n    def forward(self, y, targets, smoothing=0.1):\n        confidence = 1. - smoothing\n        log_probs = F.log_softmax(y, dim=-1) # 예측 확률 계산\n        true_probs = torch.zeros_like(log_probs)\n        true_probs.fill_(smoothing / (y.shape[1] - 1))\n        true_probs.scatter_(1, targets.data.unsqueeze(1), confidence) # 정답 인덱스의 정답 확률을 confidence로 변경\n        return torch.mean(torch.sum(true_probs * -log_probs, dim=-1)) # negative log likelihood\n\n    \n\ndevice = 'cuda'\n\nnet = resnet18()\nnet = net.to(device)\n\nlearning_rate = 0.1\n\n\ncriterion = LabelSmoothingCrossEntropy()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n\ndef train(epoch):\n    print('\\n[ Train epoch: %d ]' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n        optimizer.zero_grad()\n\n        outputs = net(inputs)\n        \n        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n        \n        loss.backward()\n\n        optimizer.step()\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n\n        total += targets.size(0)\n        current_correct = (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n        correct += current_correct\n\n        \n\n    print('\\nTotal average train accuarcy:', correct / total)\n    print('Total average train loss:', train_loss / total)\n\n\ndef test(epoch):\n    print('\\n[ Test epoch: %d ]' % epoch)\n    net.eval()\n    loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (inputs, targets) in enumerate(test_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        total += targets.size(0)\n\n        outputs = net(inputs)\n        loss += criterion(outputs, targets).item()\n\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n\n    print('\\nTotal average test accuarcy:', correct / total)\n    print('Total average test loss:', loss / total)\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    lr = learning_rate\n    if epoch >= 30:\n        lr /= 10\n    if epoch >= 60:\n        lr /= 10\n    if epoch >= 90:\n        lr /= 10\n    \n        \n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\nstart_time = time.time()\n\nfor epoch in range(0, 100):\n    adjust_learning_rate(optimizer, epoch)\n    train(epoch)\n    test(epoch)\n    print('\\nTime elapsed:', time.time() - start_time)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-15T07:30:56.738703Z","iopub.execute_input":"2021-07-15T07:30:56.739079Z","iopub.status.idle":"2021-07-15T07:46:58.649148Z","shell.execute_reply.started":"2021-07-15T07:30:56.739046Z","shell.execute_reply":"2021-07-15T07:46:58.646174Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n\n[ Train epoch: 0 ]\n\nTotal average train accuarcy: 0.2176328885213172\nTotal average train loss: 0.016197790956497194\n\n[ Test epoch: 0 ]\n\nTotal average test accuarcy: 0.3399\nTotal average test loss: 0.002655813670158386\n\nTime elapsed: 18.492364645004272\n\n[ Train epoch: 1 ]\n\nTotal average train accuarcy: 0.29457647163532913\nTotal average train loss: 0.014008414688110352\n\n[ Test epoch: 1 ]\n\nTotal average test accuarcy: 0.3804\nTotal average test loss: 0.002558593559265137\n\nTime elapsed: 36.47831201553345\n\n[ Train epoch: 2 ]\n\nTotal average train accuarcy: 0.3355159792065085\nTotal average train loss: 0.013553796668052674\n\n[ Test epoch: 2 ]\n\nTotal average test accuarcy: 0.4537\nTotal average test loss: 0.0025000219106674194\n\nTime elapsed: 54.053621768951416\n\n[ Train epoch: 3 ]\n\nTotal average train accuarcy: 0.3540017275252035\nTotal average train loss: 0.01337054853439331\n\n[ Test epoch: 3 ]\n\nTotal average test accuarcy: 0.4528\nTotal average test loss: 0.002485213375091553\n\nTime elapsed: 71.31682586669922\n\n[ Train epoch: 4 ]\n\nTotal average train accuarcy: 0.3916033331205053\nTotal average train loss: 0.013040552930831908\n\n[ Test epoch: 4 ]\n\nTotal average test accuarcy: 0.4758\nTotal average test loss: 0.002353616452217102\n\nTime elapsed: 90.10978674888611\n\n[ Train epoch: 5 ]\n\nTotal average train accuarcy: 0.4119555053758087\nTotal average train loss: 0.012861712050437927\n\n[ Test epoch: 5 ]\n\nTotal average test accuarcy: 0.464\nTotal average test loss: 0.0024465593576431274\n\nTime elapsed: 107.67173171043396\n\n[ Train epoch: 6 ]\n\nTotal average train accuarcy: 0.45003108973074407\nTotal average train loss: 0.012448716278076172\n\n[ Test epoch: 6 ]\n\nTotal average test accuarcy: 0.4562\nTotal average test loss: 0.002512323522567749\n\nTime elapsed: 125.21785950660706\n\n[ Train epoch: 7 ]\n\nTotal average train accuarcy: 0.460015550106192\nTotal average train loss: 0.012387461957931518\n\n[ Test epoch: 7 ]\n\nTotal average test accuarcy: 0.572\nTotal average test loss: 0.002204064416885376\n\nTime elapsed: 142.67155027389526\n\n[ Train epoch: 8 ]\n\nTotal average train accuarcy: 0.4780947443936654\nTotal average train loss: 0.01220306643486023\n\n[ Test epoch: 8 ]\n\nTotal average test accuarcy: 0.5446\nTotal average test loss: 0.0022284069061279297\n\nTime elapsed: 161.8957929611206\n\n[ Train epoch: 9 ]\n\nTotal average train accuarcy: 0.4879567214416422\nTotal average train loss: 0.012096776487827301\n\n[ Test epoch: 9 ]\n\nTotal average test accuarcy: 0.6139\nTotal average test loss: 0.002080334496498108\n\nTime elapsed: 179.75509214401245\n\n[ Train epoch: 10 ]\n\nTotal average train accuarcy: 0.501276341595024\nTotal average train loss: 0.011972268147468566\n\n[ Test epoch: 10 ]\n\nTotal average test accuarcy: 0.6099\nTotal average test loss: 0.002059176993370056\n\nTime elapsed: 196.6170539855957\n\n[ Train epoch: 11 ]\n\nTotal average train accuarcy: 0.5107485618446566\nTotal average train loss: 0.0118711065864563\n\n[ Test epoch: 11 ]\n\nTotal average test accuarcy: 0.6154\nTotal average test loss: 0.00207446608543396\n\nTime elapsed: 215.07740235328674\n\n[ Train epoch: 12 ]\n\nTotal average train accuarcy: 0.5150979667911509\nTotal average train loss: 0.011801518993377686\n\n[ Test epoch: 12 ]\n\nTotal average test accuarcy: 0.6938\nTotal average test loss: 0.0018923499226570128\n\nTime elapsed: 233.02001810073853\n\n[ Train epoch: 13 ]\n\nTotal average train accuarcy: 0.5274160813062879\nTotal average train loss: 0.01168207409620285\n\n[ Test epoch: 13 ]\n\nTotal average test accuarcy: 0.6459\nTotal average test loss: 0.0020045584082603453\n\nTime elapsed: 251.65304613113403\n\n[ Train epoch: 14 ]\n\nTotal average train accuarcy: 0.5252686369263682\nTotal average train loss: 0.011731341671943665\n\n[ Test epoch: 14 ]\n\nTotal average test accuarcy: 0.6153\nTotal average test loss: 0.002062736511230469\n\nTime elapsed: 269.5449159145355\n\n[ Train epoch: 15 ]\n\nTotal average train accuarcy: 0.5357586247929823\nTotal average train loss: 0.01160073679447174\n\n[ Test epoch: 15 ]\n\nTotal average test accuarcy: 0.7259\nTotal average test loss: 0.00183344806432724\n\nTime elapsed: 286.82034182548523\n\n[ Train epoch: 16 ]\n\nTotal average train accuarcy: 0.5467572000059707\nTotal average train loss: 0.011488875057697296\n\n[ Test epoch: 16 ]\n\nTotal average test accuarcy: 0.6972\nTotal average test loss: 0.0018830141663551331\n\nTime elapsed: 304.4135160446167\n\n[ Train epoch: 17 ]\n\nTotal average train accuarcy: 0.546744354325735\nTotal average train loss: 0.0114908269739151\n\n[ Test epoch: 17 ]\n\nTotal average test accuarcy: 0.7014\nTotal average test loss: 0.0018634445548057557\n\nTime elapsed: 323.2221329212189\n\n[ Train epoch: 18 ]\n\nTotal average train accuarcy: 0.5540956497043048\nTotal average train loss: 0.011425079965591431\n\n[ Test epoch: 18 ]\n\nTotal average test accuarcy: 0.7267\nTotal average test loss: 0.001854184603691101\n\nTime elapsed: 340.5577607154846\n\n[ Train epoch: 19 ]\n\nTotal average train accuarcy: 0.547533942975569\nTotal average train loss: 0.011462137050628662\n\n[ Test epoch: 19 ]\n\nTotal average test accuarcy: 0.7225\nTotal average test loss: 0.0018474488019943238\n\nTime elapsed: 358.11448669433594\n\n[ Train epoch: 20 ]\n\nTotal average train accuarcy: 0.5480020051974993\nTotal average train loss: 0.011481362464427948\n\n[ Test epoch: 20 ]\n\nTotal average test accuarcy: 0.6316\nTotal average test loss: 0.002054652142524719\n\nTime elapsed: 375.4732964038849\n\n[ Train epoch: 21 ]\n\nTotal average train accuarcy: 0.555748387209266\nTotal average train loss: 0.011342336168289184\n\n[ Test epoch: 21 ]\n\nTotal average test accuarcy: 0.6932\nTotal average test loss: 0.0018924407482147217\n\nTime elapsed: 394.66176176071167\n\n[ Train epoch: 22 ]\n\nTotal average train accuarcy: 0.5608667402194641\nTotal average train loss: 0.011315070564746857\n\n[ Test epoch: 22 ]\n\nTotal average test accuarcy: 0.6598\nTotal average test loss: 0.0019503754019737243\n\nTime elapsed: 413.1577537059784\n\n[ Train epoch: 23 ]\n\nTotal average train accuarcy: 0.5737407715419871\nTotal average train loss: 0.011206976718902587\n\n[ Test epoch: 23 ]\n\nTotal average test accuarcy: 0.759\nTotal average test loss: 0.0017419941782951356\n\nTime elapsed: 431.0382709503174\n\n[ Train epoch: 24 ]\n\nTotal average train accuarcy: 0.5814653440226657\nTotal average train loss: 0.011090239751338958\n\n[ Test epoch: 24 ]\n\nTotal average test accuarcy: 0.6839\nTotal average test loss: 0.001883128345012665\n\nTime elapsed: 450.2530279159546\n\n[ Train epoch: 25 ]\n\nTotal average train accuarcy: 0.5750544855751312\nTotal average train loss: 0.011159873540401459\n\n[ Test epoch: 25 ]\n\nTotal average test accuarcy: 0.7276\nTotal average test loss: 0.001816172170639038\n\nTime elapsed: 468.5849521160126\n\n[ Train epoch: 26 ]\n\nTotal average train accuarcy: 0.582201513501183\nTotal average train loss: 0.011069663097858428\n\n[ Test epoch: 26 ]\n\nTotal average test accuarcy: 0.7409\nTotal average test loss: 0.0017886313676834106\n\nTime elapsed: 486.03753757476807\n\n[ Train epoch: 27 ]\n\nTotal average train accuarcy: 0.582178949173923\nTotal average train loss: 0.01106035793542862\n\n[ Test epoch: 27 ]\n\nTotal average test accuarcy: 0.6739\nTotal average test loss: 0.001905143916606903\n\nTime elapsed: 503.4035596847534\n\n[ Train epoch: 28 ]\n\nTotal average train accuarcy: 0.5778280553480301\nTotal average train loss: 0.011129947319030761\n\n[ Test epoch: 28 ]\n\nTotal average test accuarcy: 0.7002\nTotal average test loss: 0.001867833149433136\n\nTime elapsed: 522.3643112182617\n\n[ Train epoch: 29 ]\n\nTotal average train accuarcy: 0.5779648803741443\nTotal average train loss: 0.011092354190349579\n\n[ Test epoch: 29 ]\n\nTotal average test accuarcy: 0.6216\nTotal average test loss: 0.0020427568197250367\n\nTime elapsed: 539.5675086975098\n\n[ Train epoch: 30 ]\n\nTotal average train accuarcy: 0.6203523362321445\nTotal average train loss: 0.010656736795902252\n\n[ Test epoch: 30 ]\n\nTotal average test accuarcy: 0.8073\nTotal average test loss: 0.0016250171661376953\n\nTime elapsed: 557.7763166427612\n\n[ Train epoch: 31 ]\n\nTotal average train accuarcy: 0.620972206035133\nTotal average train loss: 0.010635821528434753\n\n[ Test epoch: 31 ]\n\nTotal average test accuarcy: 0.8143\nTotal average test loss: 0.0016307887434959412\n\nTime elapsed: 576.1003110408783\n\n[ Train epoch: 32 ]\n\nTotal average train accuarcy: 0.6156790280700786\nTotal average train loss: 0.010640642192363738\n\n[ Test epoch: 32 ]\n\nTotal average test accuarcy: 0.8174\nTotal average test loss: 0.0016156899094581605\n\nTime elapsed: 594.1202535629272\n\n[ Train epoch: 33 ]\n\nTotal average train accuarcy: 0.6156618195118702\nTotal average train loss: 0.01064184630393982\n\n[ Test epoch: 33 ]\n\nTotal average test accuarcy: 0.8057\nTotal average test loss: 0.0016196668028831483\n\nTime elapsed: 613.003915309906\n\n[ Train epoch: 34 ]\n\nTotal average train accuarcy: 0.6436472146486568\nTotal average train loss: 0.010362559399604798\n\n[ Test epoch: 34 ]\n\nTotal average test accuarcy: 0.8177\nTotal average test loss: 0.0015845881581306458\n\nTime elapsed: 631.997053861618\n\n[ Train epoch: 35 ]\n\nTotal average train accuarcy: 0.6196440266960602\nTotal average train loss: 0.010625098161697388\n\n[ Test epoch: 35 ]\n\nTotal average test accuarcy: 0.8115\nTotal average test loss: 0.0016316617488861085\n\nTime elapsed: 649.8524262905121\n\n[ Train epoch: 36 ]\n\nTotal average train accuarcy: 0.6204294282858289\nTotal average train loss: 0.010642443256378174\n\n[ Test epoch: 36 ]\n\nTotal average test accuarcy: 0.8148\nTotal average test loss: 0.0015777254462242126\n\nTime elapsed: 667.317156791687\n\n[ Train epoch: 37 ]\n\nTotal average train accuarcy: 0.6391246668356179\nTotal average train loss: 0.010384208514690399\n\n[ Test epoch: 37 ]\n\nTotal average test accuarcy: 0.8153\nTotal average test loss: 0.0016032230734825135\n\nTime elapsed: 686.2595386505127\n\n[ Train epoch: 38 ]\n\nTotal average train accuarcy: 0.6185270914027119\nTotal average train loss: 0.01062263525724411\n\n[ Test epoch: 38 ]\n\nTotal average test accuarcy: 0.8191\nTotal average test loss: 0.0015783986330032349\n\nTime elapsed: 703.6330518722534\n\n[ Train epoch: 39 ]\n\nTotal average train accuarcy: 0.6279420404618219\nTotal average train loss: 0.01050595299243927\n\n[ Test epoch: 39 ]\n\nTotal average test accuarcy: 0.8237\nTotal average test loss: 0.0015752010464668274\n\nTime elapsed: 721.5648443698883\n\n[ Train epoch: 40 ]\n\nTotal average train accuarcy: 0.6168869655088978\nTotal average train loss: 0.01064789824962616\n\n[ Test epoch: 40 ]\n\nTotal average test accuarcy: 0.8116\nTotal average test loss: 0.0016273489117622376\n\nTime elapsed: 739.3487305641174\n\n[ Train epoch: 41 ]\n\nTotal average train accuarcy: 0.6533131821604701\nTotal average train loss: 0.010252186872959137\n\n[ Test epoch: 41 ]\n\nTotal average test accuarcy: 0.8183\nTotal average test loss: 0.0016107892870903015\n\nTime elapsed: 758.1497416496277\n\n[ Train epoch: 42 ]\n\nTotal average train accuarcy: 0.6281380362254859\nTotal average train loss: 0.010525272755622863\n\n[ Test epoch: 42 ]\n\nTotal average test accuarcy: 0.8229\nTotal average test loss: 0.0015831335425376893\n\nTime elapsed: 776.5145041942596\n\n[ Train epoch: 43 ]\n\nTotal average train accuarcy: 0.640569168313485\nTotal average train loss: 0.010326447958946229\n\n[ Test epoch: 43 ]\n\nTotal average test accuarcy: 0.827\nTotal average test loss: 0.0015746660590171813\n\nTime elapsed: 793.740268945694\n\n[ Train epoch: 44 ]\n\nTotal average train accuarcy: 0.6356377199054523\nTotal average train loss: 0.010363202669620514\n\n[ Test epoch: 44 ]\n\nTotal average test accuarcy: 0.8221\nTotal average test loss: 0.001576375985145569\n\nTime elapsed: 812.7340247631073\n\n[ Train epoch: 45 ]\n\nTotal average train accuarcy: 0.6449144122499305\nTotal average train loss: 0.010308843328952788\n\n[ Test epoch: 45 ]\n\nTotal average test accuarcy: 0.8264\nTotal average test loss: 0.0015373659133911134\n\nTime elapsed: 830.8305633068085\n\n[ Train epoch: 46 ]\n\nTotal average train accuarcy: 0.6438172191542875\nTotal average train loss: 0.010307428493499755\n\n[ Test epoch: 46 ]\n\nTotal average test accuarcy: 0.8259\nTotal average test loss: 0.001585345232486725\n\nTime elapsed: 848.592206954956\n\n[ Train epoch: 47 ]\n\nTotal average train accuarcy: 0.6569776637340438\nTotal average train loss: 0.010150069077014923\n\n[ Test epoch: 47 ]\n\nTotal average test accuarcy: 0.8248\nTotal average test loss: 0.0015621886014938354\n\nTime elapsed: 866.3000428676605\n\n[ Train epoch: 48 ]\n\nTotal average train accuarcy: 0.6373346488981182\nTotal average train loss: 0.010400993993282318\n\n[ Test epoch: 48 ]\n\nTotal average test accuarcy: 0.8263\nTotal average test loss: 0.0015813182473182678\n\nTime elapsed: 885.996490240097\n\n[ Train epoch: 49 ]\n\nTotal average train accuarcy: 0.6580469833671775\nTotal average train loss: 0.010168272123336791\n\n[ Test epoch: 49 ]\n\nTotal average test accuarcy: 0.8214\nTotal average test loss: 0.0015770183801651\n\nTime elapsed: 903.8838925361633\n\n[ Train epoch: 50 ]\n\nTotal average train accuarcy: 0.6309556284011367\nTotal average train loss: 0.010472483468055726\n\n[ Test epoch: 50 ]\n\nTotal average test accuarcy: 0.8233\nTotal average test loss: 0.001560590648651123\n\nTime elapsed: 922.8080191612244\n\n[ Train epoch: 51 ]\n\nTotal average train accuarcy: 0.6368941384859436\nTotal average train loss: 0.010448938648700714\n\n[ Test epoch: 51 ]\n\nTotal average test accuarcy: 0.8252\nTotal average test loss: 0.0015832404136657714\n\nTime elapsed: 941.0876693725586\n\n[ Train epoch: 52 ]\n\nTotal average train accuarcy: 0.6538992143180138\nTotal average train loss: 0.010270732281208039\n\n[ Test epoch: 52 ]\n\nTotal average test accuarcy: 0.8253\nTotal average test loss: 0.0015799542427062988\n\nTime elapsed: 958.2497494220734\n\n[ Train epoch: 53 ]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-b087d07a0124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTime elapsed:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-b087d07a0124>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}